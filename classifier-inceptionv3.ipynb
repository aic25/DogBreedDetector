{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import GPUtil\n",
    "import matplotlib.pyplot as plt                        \n",
    "%matplotlib inline  \n",
    "\n",
    "# In a multi GPU server, chose which to use:\n",
    "NUMBER_OF_GPUS_TO_USE = 1\n",
    "Availability=GPUtil.getAvailability(GPUtil.getGPUs())\n",
    "all_gpus = np.arange(3)\n",
    "available_gpu_indexes = [x for x in all_gpus if Availability[x]]\n",
    "# Set CUDA_DEVICE_ORDER so the IDs assigned by CUDA match those from nvidia-smi\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "# Set CUDA_VISIBLE_DEVICES to mask out all other GPUs than the first NUMBER_OF_GPUS_TO_USE available device id\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = ','.join(np.array(available_gpu_indexes[:NUMBER_OF_GPUS_TO_USE]).astype(str))\n",
    "\n",
    "import tensorflow as tf\n",
    "config=tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth=True))\n",
    "\n",
    "import keras\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "\n",
    "set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if its a human face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract pre-trained face detector\n",
    "face_cascade = cv2.CascadeClassifier('haarcascades/haarcascade_frontalface_alt.xml')\n",
    "# returns \"True\" if face is detected in image stored at img_path\n",
    "def face_detector(img_path):\n",
    "    img = cv2.imread(img_path)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray)\n",
    "    return len(faces) > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confirm the image is actually a dog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ability of this process is limited by the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.preprocessing import image                  \n",
    "from tqdm import tqdm\n",
    "\n",
    "# define ResNet50 model\n",
    "ResNet50_model = ResNet50(weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def path_to_tensor(img_path, expand=True, **kwargs):\n",
    "    # loads RGB image as PIL.Image.Image type\n",
    "    img = image.load_img(img_path, **kwargs)\n",
    "    # convert PIL.Image.Image type to 3D tensor with shape (224, 224, 3)\n",
    "    x = image.img_to_array(img)\n",
    "    # convert 3D tensor to 4D tensor with shape (1, 224, 224, 3) and return 4D tensor\n",
    "    if expand:\n",
    "        return np.expand_dims(x, axis=0)\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "def paths_to_tensor(img_paths, **kwargs):\n",
    "    list_of_tensors = [path_to_tensor(img_path, **kwargs) for img_path in img_paths]\n",
    "    return np.vstack(list_of_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "\n",
    "def ResNet50_predict_labels(img_path):\n",
    "    # returns prediction vector for image located at img_path\n",
    "    img = preprocess_input(path_to_tensor(img_path))\n",
    "    return np.argmax(ResNet50_model.predict(img))\n",
    "\n",
    "### returns \"True\" if a dog is detected in the image stored at img_path\n",
    "def dog_detector(img_path):\n",
    "    prediction = ResNet50_predict_labels(img_path)\n",
    "    return ((prediction <= 268) & (prediction >= 151)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use transfer learning to reduce training time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define your architecture.\n",
    "from keras.models import Sequential\n",
    "from keras.layers.pooling import GlobalAveragePooling2D\n",
    "from keras.layers.core import Dense\n",
    "\n",
    "Resnet50_model = Sequential()\n",
    "Resnet50_model.add(GlobalAveragePooling2D(input_shape=(7, 7, 2048)))\n",
    "Resnet50_model.add(Dense(120, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "global_average_pooling2d_5 ( (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 120)               245880    \n",
      "=================================================================\n",
      "Total params: 245,880\n",
      "Trainable params: 245,880\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "Resnet50_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compile and test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_files       \n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "from glob import glob\n",
    "\n",
    "data = load_files(\"/notebooks/Practice/imgnet-dogbreed/Images/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['DESCR', 'target_names', 'target', 'data', 'filenames'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_files, test_files, train_targets, test_targets = train_test_split(data['filenames'], data['target'], test_size=0.2)\n",
    "test_files, validation_files, test_targets, validation_targets = train_test_split(test_files, test_targets, test_size=0.5)\n",
    "train_targets = np_utils.to_categorical(np.array(train_targets), 120)\n",
    "test_targets = np_utils.to_categorical(np.array(test_targets), 120)\n",
    "validation_targets = np_utils.to_categorical(np.array(validation_targets), 120)\n",
    "\n",
    "dog_names = [item.split('/')[-1] for item in sorted(glob(\"/notebooks/Practice/imgnet-dogbreed/Images/*\"))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/notebooks/Practice/imgnet-dogbreed/Images/n02097474-Tibetan_terrier/n02097474_4929.jpg'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_files[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(test_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'n02093859-Kerry_blue_terrier'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['target_names'][32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 120 total dog categories.\n",
      "There are 20580 total dog images.\n",
      "\n",
      "There are 16464 training dog images.\n",
      "There are 2058 test dog images.\n",
      "There are 2058 test dog images.\n"
     ]
    }
   ],
   "source": [
    "# print statistics about the dataset\n",
    "print('There are %d total dog categories.' % len(dog_names))\n",
    "print('There are %s total dog images.\\n' % len(np.hstack([train_files, validation_files, test_files])))\n",
    "print('There are %d training dog images.' % len(train_files))\n",
    "print('There are %d test dog images.'% len(validation_files))\n",
    "print('There are %d test dog images.'% len(test_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create bottleneck features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "### Obtain bottleneck features from another pre-trained CNN.\n",
    "bottleneck_features = np.load('AIND2-dog-project/bottleneck_features/DogResnet50Data.npz')\n",
    "train_DogResnet50 = bottleneck_features['train']\n",
    "valid_DogResnet50 = bottleneck_features['valid']\n",
    "test_DogResnet50 = bottleneck_features['test']\n",
    "\n",
    "set(zip(bottleneck_features.keys(), [np.shape(bottleneck_features[key]) for key in bottleneck_features.keys()]))\n",
    "\n",
    "{('filenames_test', (836,)),\n",
    " ('filenames_train', (6680,)),\n",
    " ('filenames_valid', (835,)),\n",
    " ('test', (836, 1, 1, 2048)),\n",
    " ('test_OH', (836, 133)),\n",
    " ('train', (6680, 1, 1, 2048)),\n",
    " ('train_OH', (6680, 133)),\n",
    " ('valid', (835, 1, 1, 2048)),\n",
    " ('valid_OH', (835, 133))}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.engine.input_layer import Input\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_img = Input(shape=(360, 360, 3))\n",
    "\n",
    "model_Inception = keras.applications.InceptionV3(\n",
    "    include_top=True,weights='imagenet',\n",
    "    input_shape=(360, 360, 3),\n",
    "    input_tensor=input_img)\n",
    "extracted_features = model_Inception.layers[-1].input\n",
    "extraction_model = Model(inputs=input_img,outputs=extracted_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 34s, sys: 55.6 s, total: 4min 29s\n",
      "Wall time: 4min 19s\n"
     ]
    }
   ],
   "source": [
    "%time train_features = extraction_model.predict(preprocess_input(paths_to_tensor(train_files, target_size=(360, 360))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 26.8 s, sys: 5.51 s, total: 32.3 s\n",
      "Wall time: 31.1 s\n"
     ]
    }
   ],
   "source": [
    "%time test_features = extraction_model.predict(preprocess_input(paths_to_tensor(test_files, target_size=(360, 360))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 26.3 s, sys: 5.11 s, total: 31.4 s\n",
      "Wall time: 30.1 s\n"
     ]
    }
   ],
   "source": [
    "%time validation_features = extraction_model.predict(preprocess_input(paths_to_tensor(validation_files, target_size=(360, 360))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(16464, 2048), (2058, 2048), (2058, 2048)],\n",
       " [(16464, 120), (2058, 120), (2058, 120)]]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[[np.shape(train_features), np.shape(test_features), np.shape(validation_features)],\n",
    " [np.shape(train_targets), np.shape(test_targets), np.shape(validation_targets)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir InceptionV3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = np.load('InceptionV3/train_features.npy')\n",
    "test_features = np.load('InceptionV3/test_features.npy')\n",
    "valid_features = np.load('InceptionV3/validation_features.npy')\n",
    "\n",
    "train_targets = np.load('InceptionV3/train_targets.npy')\n",
    "test_targets = np.load('InceptionV3/test_targets.npy')\n",
    "validation_targets = np.load('InceptionV3/validation_targets.npy')\n",
    "\n",
    "train_files = np.load('InceptionV3/train_files.npy')\n",
    "test_files = np.load('InceptionV3/test_files.npy')\n",
    "validation_files = np.load('InceptionV3/validation_files.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('InceptionV3/train_features', train_features)\n",
    "np.save('InceptionV3/test_features', test_features)\n",
    "np.save('InceptionV3/validation_features', validation_features)\n",
    "np.save('InceptionV3/train_targets', train_targets)\n",
    "np.save('InceptionV3/test_targets', test_targets)\n",
    "np.save('InceptionV3/validation_targets', validation_targets)\n",
    "\n",
    "np.save('InceptionV3/train_files',train_files)\n",
    "np.save('InceptionV3/test_files',test_files)\n",
    "np.save('InceptionV3/validation_files',validation_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers.core import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_features = Input(shape=(2048,))\n",
    "out = Dense(units=120,activation='softmax',use_bias=True)(input_features)\n",
    "Iv3_model = Model(inputs=input_features,outputs=out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f989c25fc50>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Iv3_model.compile(optimizer=keras.optimizers.Adam(lr=0.01),\n",
    "              loss=keras.losses.categorical_crossentropy,\n",
    "              metrics=[keras.metrics.categorical_accuracy])\n",
    "checkpointer_iv3 = ModelCheckpoint(filepath='InceptionV3/weights.best.InceptionV3.hdf5', \n",
    "                               verbose=0, save_best_only=True)\n",
    "Iv3_model.fit(train_features, train_targets, \n",
    "      validation_data=(validation_features, validation_targets),\n",
    "      epochs=200, batch_size=200, callbacks=[checkpointer_iv3], verbose=0)\n",
    "\n",
    "# np.expand_dims(np.expand_dims(validation_targets,1),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load the model weights with the best validation loss.\n",
    "Iv3_model.load_weights('InceptionV3/weights.best.InceptionV3.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.9232%\n"
     ]
    }
   ],
   "source": [
    "### Calculate classification accuracy on the test dataset.\n",
    "predictions = np.argmax(Iv3_model.predict(test_features), axis=1)\n",
    "actual = np.argmax(test_targets, axis=1)\n",
    "# Report test accuracy\n",
    "test_accuracy = 100*np.sum(predictions==actual)/len(predictions)\n",
    "print('Test accuracy: %.4f%%' % test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
