{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import GPUtil\n",
    "import matplotlib.pyplot as plt                        \n",
    "%matplotlib inline  \n",
    "\n",
    "# In a multi GPU server, chose which to use:\n",
    "NUMBER_OF_GPUS_TO_USE = 1\n",
    "Availability=GPUtil.getAvailability(GPUtil.getGPUs())\n",
    "all_gpus = np.arange(3)\n",
    "available_gpu_indexes = [x for x in all_gpus if Availability[x]]\n",
    "# Set CUDA_DEVICE_ORDER so the IDs assigned by CUDA match those from nvidia-smi\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "# Set CUDA_VISIBLE_DEVICES to mask out all other GPUs than the first NUMBER_OF_GPUS_TO_USE available device id\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = ','.join(np.array(available_gpu_indexes[:NUMBER_OF_GPUS_TO_USE]).astype(str))\n",
    "\n",
    "import tensorflow as tf\n",
    "config=tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth=True))\n",
    "\n",
    "import keras\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "\n",
    "set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.preprocessing import image                  \n",
    "from tqdm import tqdm\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.pooling import GlobalAveragePooling2D\n",
    "from keras.layers.core import Dense\n",
    "\n",
    "from keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "\n",
    "from sklearn.datasets import load_files       \n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "from glob import glob\n",
    "\n",
    "from keras.engine.input_layer import Input\n",
    "from keras.models import Model\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string, random\n",
    "def id_generator(size=6, chars=string.ascii_uppercase + string.digits):\n",
    "    return ''.join(random.choice(chars) for _ in range(size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if its a human face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract pre-trained face detector\n",
    "face_cascade = cv2.CascadeClassifier('haarcascades/haarcascade_frontalface_alt.xml')\n",
    "# returns \"True\" if face is detected in image stored at img_path\n",
    "def face_detector(img_path):\n",
    "    img = cv2.imread(img_path)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray)\n",
    "    return len(faces) > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confirm the image is actually a dog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ability of this process is limited by the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define ResNet50 model\n",
    "ResNet50_model = ResNet50(weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def path_to_tensor(img_path, expand=True):\n",
    "    # loads RGB image as PIL.Image.Image type\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    # convert PIL.Image.Image type to 3D tensor with shape (224, 224, 3)\n",
    "    x = image.img_to_array(img)\n",
    "    # convert 3D tensor to 4D tensor with shape (1, 224, 224, 3) and return 4D tensor\n",
    "    if expand:\n",
    "        return np.expand_dims(x, axis=0)\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "def paths_to_tensor(img_paths):\n",
    "    list_of_tensors = [path_to_tensor(img_path) for img_path in img_paths]\n",
    "    return np.vstack(list_of_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ResNet50_predict_labels(img_path):\n",
    "    # returns prediction vector for image located at img_path\n",
    "    img = preprocess_input(path_to_tensor(img_path))\n",
    "    return np.argmax(ResNet50_model.predict(img))\n",
    "\n",
    "### returns \"True\" if a dog is detected in the image stored at img_path\n",
    "def dog_detector(img_path):\n",
    "    prediction = ResNet50_predict_labels(img_path)\n",
    "    return ((prediction <= 268) & (prediction >= 151)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use transfer learning to reduce training time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define architecture.\n",
    "Resnet50_model = Sequential()\n",
    "Resnet50_model.add(GlobalAveragePooling2D(input_shape=(7, 7, 2048)))\n",
    "Resnet50_model.add(Dense(120, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "global_average_pooling2d_1 ( (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 120)               245880    \n",
      "=================================================================\n",
      "Total params: 245,880\n",
      "Trainable params: 245,880\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "Resnet50_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compile and test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_files(\"/notebooks/Practice/imgnet-dogbreed/Images/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_files, test_files, train_targets, test_targets = train_test_split(data['filenames'], data['target'], test_size=0.2)\n",
    "test_files, validation_files, test_targets, validation_targets = train_test_split(test_files, test_targets, test_size=0.5)\n",
    "train_targets = np_utils.to_categorical(np.array(train_targets), 120)\n",
    "test_targets = np_utils.to_categorical(np.array(test_targets), 120)\n",
    "validation_targets = np_utils.to_categorical(np.array(validation_targets), 120)\n",
    "\n",
    "dog_names = [item.split('/')[-1] for item in sorted(glob(\"/notebooks/Practice/imgnet-dogbreed/Images/*\"))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 120 total dog categories.\n",
      "There are 20580 total dog images.\n",
      "\n",
      "There are 16464 training dog images.\n",
      "There are 2058 test dog images.\n",
      "There are 2058 test dog images.\n"
     ]
    }
   ],
   "source": [
    "# print statistics about the dataset\n",
    "print('There are %d total dog categories.' % len(dog_names))\n",
    "print('There are %s total dog images.\\n' % len(np.hstack([train_files, validation_files, test_files])))\n",
    "print('There are %d training dog images.' % len(train_files))\n",
    "print('There are %d test dog images.'% len(validation_files))\n",
    "print('There are %d test dog images.'% len(test_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create bottleneck features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = np.load('ResNet50/train_features.npy')\n",
    "test_features = np.load('ResNet50/test_features.npy')\n",
    "validation_features = np.load('ResNet50/validation_features.npy')\n",
    "\n",
    "train_targets = np.load('ResNet50/train_targets.npy')\n",
    "test_targets = np.load('ResNet50/test_targets.npy')\n",
    "validation_targets = np.load('ResNet50/validation_targets.npy')\n",
    "\n",
    "train_files = np.load('ResNet50/train_files.npy')\n",
    "test_files = np.load('ResNet50/test_files.npy')\n",
    "validation_files = np.load('ResNet50/validation_files.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_img = Input(shape=(224, 224, 3))\n",
    "\n",
    "model_resnet = ResNet50(weights='imagenet', include_top=False,\n",
    "    input_shape=(224, 224, 3),\n",
    "    input_tensor=input_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 47s, sys: 28.8 s, total: 3min 15s\n",
      "Wall time: 3min 9s\n"
     ]
    }
   ],
   "source": [
    "%time train_features = model_resnet.predict(preprocess_input(paths_to_tensor(train_files)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 20.4 s, sys: 3.01 s, total: 23.4 s\n",
      "Wall time: 22.6 s\n"
     ]
    }
   ],
   "source": [
    "%time test_features = model_resnet.predict(preprocess_input(paths_to_tensor(test_files)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 20.4 s, sys: 2.94 s, total: 23.3 s\n",
      "Wall time: 22.5 s\n"
     ]
    }
   ],
   "source": [
    "%time validation_features = model_resnet.predict(preprocess_input(paths_to_tensor(validation_files)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(16464, 7, 7, 2048), (2058, 7, 7, 2048), (2058, 7, 7, 2048)],\n",
       " [(16464, 120), (2058, 120), (2058, 120)]]"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[[np.shape(train_features), np.shape(test_features), np.shape(validation_features)],\n",
    " [np.shape(train_targets), np.shape(test_targets), np.shape(validation_targets)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('ResNet50/train_features', train_features)\n",
    "np.save('ResNet50/test_features', test_features)\n",
    "np.save('ResNet50/validation_features', validation_features)\n",
    "np.save('ResNet50/train_targets', train_targets)\n",
    "np.save('ResNet50/test_targets', test_targets)\n",
    "np.save('ResNet50/validation_targets', validation_targets)\n",
    "\n",
    "np.save('ResNet50/train_files',train_files)\n",
    "np.save('ResNet50/test_files',test_files)\n",
    "np.save('ResNet50/validation_files',validation_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Compile the model.\n",
    "Resnet50_model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "Resnet50_model.compile(optimizer=keras.optimizers.Nadam(),\n",
    "                  loss=keras.losses.categorical_crossentropy,\n",
    "                  metrics=[keras.metrics.categorical_accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Train the model.\n",
    "checkpointer_resnet = ModelCheckpoint(filepath='ResNet50/weights.best.ResNet50.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "\n",
    "tb = keras.callbacks.TensorBoard(log_dir='./logs'+'/'+id_generator(), histogram_freq=0, batch_size=32, write_graph=True, write_grads=False, write_images=False, embeddings_freq=0, embeddings_layer_names=None, embeddings_metadata=None, embeddings_data=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_loss improved from inf to 2.97892, saving model to ResNet50/weights.best.ResNet50.hdf5\n",
      "\n",
      "Epoch 00002: val_loss improved from 2.97892 to 2.96905, saving model to ResNet50/weights.best.ResNet50.hdf5\n",
      "\n",
      "Epoch 00003: val_loss improved from 2.96905 to 2.91374, saving model to ResNet50/weights.best.ResNet50.hdf5\n",
      "\n",
      "Epoch 00004: val_loss improved from 2.91374 to 2.89610, saving model to ResNet50/weights.best.ResNet50.hdf5\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 2.89610\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 2.89610\n",
      "\n",
      "Epoch 00007: val_loss improved from 2.89610 to 2.79224, saving model to ResNet50/weights.best.ResNet50.hdf5\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 2.79224\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 2.79224\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 2.79224\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 2.79224\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 2.79224\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 2.79224\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 2.79224\n",
      "\n",
      "Epoch 00015: val_loss improved from 2.79224 to 2.74174, saving model to ResNet50/weights.best.ResNet50.hdf5\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 2.74174\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 2.74174\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 2.74174\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 2.74174\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 2.74174\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 2.74174\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 2.74174\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 2.74174\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 2.74174\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 2.74174\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 2.74174\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 2.74174\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 2.74174\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 2.74174\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 2.74174\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 2.74174\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 2.74174\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 2.74174\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 2.74174\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 2.74174\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 2.74174\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 2.74174\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 2.74174\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 2.74174\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 2.74174\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 2.74174\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 2.74174\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 2.74174\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 2.74174\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 2.74174\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 2.74174\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 2.74174\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 2.74174\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 2.74174\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 2.74174\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 2.74174\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 2.74174\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 2.74174\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 2.74174\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 2.74174\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 2.74174\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 2.74174\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 2.74174\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 2.74174\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 2.74174\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 2.74174\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 2.74174\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 2.74174\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 2.74174\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 2.74174\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 2.74174\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 2.74174\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 2.74174\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 2.74174\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 2.74174\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 2.74174\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 2.74174\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-10bae14fc0c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m Resnet50_model.fit(train_features, train_targets, \n\u001b[1;32m      2\u001b[0m           \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_targets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m           epochs=500, batch_size=50, callbacks=[checkpointer_resnet, tb], verbose = 0)\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1040\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1042\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1043\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1044\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2659\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2660\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2661\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2662\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2663\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2629\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2630\u001b[0m                                 session)\n\u001b[0;32m-> 2631\u001b[0;31m         \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2632\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1449\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_with_new_api\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1450\u001b[0m           return tf_session.TF_SessionRunCallable(\n\u001b[0;32m-> 1451\u001b[0;31m               self._session._session, self._handle, args, status, None)\n\u001b[0m\u001b[1;32m   1452\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1453\u001b[0m           return tf_session.TF_DeprecatedSessionRunCallable(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "Resnet50_model.fit(train_features, train_targets, \n",
    "          validation_data=(validation_features, validation_targets),\n",
    "          epochs=50, batch_size=50, callbacks=[checkpointer_resnet, tb], verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load the model weights with the best validation loss.\n",
    "Resnet50_model.load_weights('ResNet50/weights.best.ResNet50.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 74.8299%\n"
     ]
    }
   ],
   "source": [
    "### Calculate classification accuracy on the test dataset.\n",
    "predictions = np.argmax(Resnet50_model.predict(test_features), axis=1)\n",
    "actual = np.argmax(test_targets, axis=1)\n",
    "# Report test accuracy\n",
    "test_accuracy = 100*np.sum(predictions==actual)/len(predictions)\n",
    "print('Test accuracy: %.4f%%' % test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 75.6074%\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
