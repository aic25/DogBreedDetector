{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import GPUtil\n",
    "import matplotlib.pyplot as plt                        \n",
    "%matplotlib inline  \n",
    "\n",
    "# In a multi GPU server, chose which to use:\n",
    "NUMBER_OF_GPUS_TO_USE = 1\n",
    "Availability=GPUtil.getAvailability(GPUtil.getGPUs())\n",
    "all_gpus = np.arange(3)\n",
    "available_gpu_indexes = [x for x in all_gpus if Availability[x]]\n",
    "# Set CUDA_DEVICE_ORDER so the IDs assigned by CUDA match those from nvidia-smi\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "# Set CUDA_VISIBLE_DEVICES to mask out all other GPUs than the first NUMBER_OF_GPUS_TO_USE available device id\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = ','.join(np.array(available_gpu_indexes[:NUMBER_OF_GPUS_TO_USE]).astype(str))\n",
    "\n",
    "import tensorflow as tf\n",
    "config=tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth=True))\n",
    "\n",
    "import keras\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "\n",
    "set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.preprocessing import image                  \n",
    "from tqdm import tqdm\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.pooling import GlobalAveragePooling2D\n",
    "from keras.layers.core import Dense\n",
    "\n",
    "from keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "\n",
    "from sklearn.datasets import load_files       \n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "from glob import glob\n",
    "\n",
    "from keras.engine.input_layer import Input\n",
    "from keras.models import Model\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string, random\n",
    "def id_generator(size=6, chars=string.ascii_uppercase + string.digits):\n",
    "    return ''.join(random.choice(chars) for _ in range(size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confirm the image is actually a dog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ability of this process is limited by the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def path_to_tensor(img_path, expand=True):\n",
    "    # loads RGB image as PIL.Image.Image type\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    # convert PIL.Image.Image type to 3D tensor with shape (224, 224, 3)\n",
    "    x = image.img_to_array(img)\n",
    "    # convert 3D tensor to 4D tensor with shape (1, 224, 224, 3) and return 4D tensor\n",
    "    if expand:\n",
    "        return np.expand_dims(x, axis=0)\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "def paths_to_tensor(img_paths):\n",
    "    list_of_tensors = [path_to_tensor(img_path) for img_path in img_paths]\n",
    "    return np.vstack(list_of_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define ResNet50 model\n",
    "ResNet50_model = ResNet50(weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ResNet50_predict_labels(img_path):\n",
    "    # returns prediction vector for image located at img_path\n",
    "    img = preprocess_input(path_to_tensor(img_path))\n",
    "    return np.argmax(ResNet50_model.predict(img))\n",
    "\n",
    "### returns \"True\" if a dog is detected in the image stored at img_path\n",
    "def dog_detector(img_path):\n",
    "    prediction = ResNet50_predict_labels(img_path)\n",
    "    return ((prediction <= 268) & (prediction >= 151)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use transfer learning to reduce training time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define architecture.\n",
    "Resnet50_model = Sequential()\n",
    "Resnet50_model.add(GlobalAveragePooling2D(input_shape=(7, 7, 2048)))\n",
    "Resnet50_model.add(Dense(120, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "global_average_pooling2d_1 ( (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 120)               245880    \n",
      "=================================================================\n",
      "Total params: 245,880\n",
      "Trainable params: 245,880\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "Resnet50_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compile and test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_files(\"/notebooks/Practice/imgnet-dogbreed/Images/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_files, test_files, train_targets, test_targets = train_test_split(data['filenames'], data['target'], test_size=0.1)\n",
    "train_targets = np_utils.to_categorical(np.array(train_targets), 120)\n",
    "test_targets = np_utils.to_categorical(np.array(test_targets), 120)\n",
    "\n",
    "dog_names = [item.split('/')[-1] for item in sorted(glob(\"/notebooks/Practice/imgnet-dogbreed/Images/*\"))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 120 total dog categories.\n",
      "There are 20580 total dog images.\n",
      "\n",
      "There are 18522 training dog images.\n",
      "There are 2058 test dog images.\n"
     ]
    }
   ],
   "source": [
    "# print statistics about the dataset\n",
    "print('There are %d total dog categories.' % len(dog_names))\n",
    "print('There are %s total dog images.\\n' % len(np.hstack([train_files, test_files])))\n",
    "print('There are %d training dog images.' % len(train_files))\n",
    "print('There are %d test dog images.'% len(test_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create bottleneck features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_crop(image):\n",
    "    \"\"\"Crops a random region of width x height from image.\n",
    "    Returns an image\"\"\"\n",
    "    ratio = 0.8\n",
    "    width = int(image.shape[1] * ratio)\n",
    "    height = int(image.shape[0] * ratio)\n",
    "    crop_origin_x = np.random.randint(0, image.shape[0] - width)\n",
    "    crop_origin_y = np.random.randint(0, image.shape[1] - height)\n",
    "    cropped_image = image[crop_origin_y:crop_origin_y+height,crop_origin_x:crop_origin_x+width,:]\n",
    "    cropped_image = cv2.resize(cropped_image,(image.shape[0],image.shape[1]))\n",
    "    return cropped_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and configure augmented image generator\n",
    "datagen_train = ImageDataGenerator(preprocessing_function=random_crop,\n",
    "                                   rotation_range=45,\n",
    "                                   width_shift_range=0.1,\n",
    "                                   height_shift_range=0.1,\n",
    "                                   shear_range=0.1,\n",
    "                                   zoom_range=0.1,\n",
    "                                   horizontal_flip=True,\n",
    "                                   fill_mode='nearest')\n",
    "\n",
    "# create and configure augmented image generator\n",
    "datagen_test = ImageDataGenerator() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_train_files = preprocess_input(paths_to_tensor(train_files))\n",
    "processed_test_files = preprocess_input(paths_to_tensor(test_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_targets = np.load('ResNet50/train_targets.npy')\n",
    "test_targets = np.load('ResNet50/test_targets.npy')\n",
    "\n",
    "train_files = np.load('ResNet50/train_files.npy')\n",
    "test_files = np.load('ResNet50/test_files.npy')\n",
    "\n",
    "processed_train_files = np.load('ResNet50/processed_train_files.npy')\n",
    "processed_test_files = np.load('ResNet50/processed_test_files.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit augmented image generator on data\n",
    "datagen_train.fit(processed_train_files)\n",
    "datagen_test.fit(processed_test_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_img = Input(shape=(224, 224, 3))\n",
    "\n",
    "model_resnet = ResNet50(weights='imagenet', include_top=False,\n",
    "    input_shape=(224, 224, 3),\n",
    "    input_tensor=input_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_flow = datagen_train.flow(processed_train_files, train_targets, shuffle=True, batch_size=3000)\n",
    "test_flow = datagen_test.flow(processed_test_files, test_targets, shuffle=False, batch_size=len(test_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "Resnet50_model.compile(optimizer=keras.optimizers.Nadam(),\n",
    "                  loss=keras.losses.categorical_crossentropy,\n",
    "                  metrics=[keras.metrics.categorical_accuracy])\n",
    "### Train the model.\n",
    "checkpointer_resnet = ModelCheckpoint(filepath='ResNet50/weights.best.ResNet50.hdf5', \n",
    "                               verbose=0, save_best_only=True)\n",
    "\n",
    "tb = keras.callbacks.TensorBoard(log_dir='./logs'+'/'+id_generator(), histogram_freq=0, batch_size=32, write_graph=True, write_grads=False, write_images=False, embeddings_freq=0, embeddings_layer_names=None, embeddings_metadata=None, embeddings_data=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x,y in train_flow:\n",
    "    history = Resnet50_model.fit(model_resnet.predict(x), y, epochs=5, callbacks=[checkpointer_resnet, tb], validation_split=.1, verbose=0)\n",
    "    if history.history['val_categorical_accuracy'][0] > .9:\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "test_features = model_resnet.predict(processed_test_files)\n",
    "\n",
    "### Calculate classification accuracy on the test dataset.\n",
    "predictions = np.argmax(Resnet50_model.predict(test_features), axis=1)\n",
    "actual = np.argmax(test_targets, axis=1)\n",
    "# Report test accuracy\n",
    "test_accuracy = 100*np.sum(predictions==actual)/len(predictions)\n",
    "print('Test accuracy: %.4f%%' % test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('ResNet50/train_targets', train_targets)\n",
    "np.save('ResNet50/test_targets', test_targets)\n",
    "\n",
    "np.save('ResNet50/train_files',train_files)\n",
    "np.save('ResNet50/test_files',test_files)\n",
    "\n",
    "np.save('ResNet50/processed_train_files', processed_train_files)\n",
    "np.save('ResNet50/processed_test_files', processed_test_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 224, 224, 3) (32, 120)\n",
      "(32, 224, 224, 3) (32, 120)\n",
      "(32, 224, 224, 3) (32, 120)\n",
      "(32, 224, 224, 3) (32, 120)\n",
      "(32, 224, 224, 3) (32, 120)\n",
      "(32, 224, 224, 3) (32, 120)\n",
      "(32, 224, 224, 3) (32, 120)\n",
      "(32, 224, 224, 3) (32, 120)\n",
      "(32, 224, 224, 3) (32, 120)\n",
      "(32, 224, 224, 3) (32, 120)\n",
      "(32, 224, 224, 3) (32, 120)\n",
      "(32, 224, 224, 3) (32, 120)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-5a710a145b91>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalidation_flow\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras_preprocessing/image.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1297\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1298\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1300\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_batches_of_transformed_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras_preprocessing/image.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1463\u001b[0m         \u001b[0;31m# The transformation of images is not under thread lock\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1464\u001b[0m         \u001b[0;31m# so it can be done in parallel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1465\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_batches_of_transformed_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1466\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1467\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras_preprocessing/image.py\u001b[0m in \u001b[0;36m_get_batches_of_transformed_samples\u001b[0;34m(self, index_array)\u001b[0m\n\u001b[1;32m   1428\u001b[0m             \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_data_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_random_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1429\u001b[0m             x = self.image_data_generator.apply_transform(\n\u001b[0;32m-> 1430\u001b[0;31m                 x.astype(backend.floatx()), params)\n\u001b[0m\u001b[1;32m   1431\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_data_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstandardize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1432\u001b[0m             \u001b[0mbatch_x\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras_preprocessing/image.py\u001b[0m in \u001b[0;36mapply_transform\u001b[0;34m(self, x, transform_parameters)\u001b[0m\n\u001b[1;32m   1112\u001b[0m                                    \u001b[0mrow_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimg_row_axis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimg_col_axis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m                                    \u001b[0mchannel_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimg_channel_axis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1114\u001b[0;31m                                    fill_mode=self.fill_mode, cval=self.cval)\n\u001b[0m\u001b[1;32m   1115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtransform_parameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'channel_shift_intensity'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras_preprocessing/image.py\u001b[0m in \u001b[0;36mapply_affine_transform\u001b[0;34m(x, theta, tx, ty, shear, zx, zy, row_axis, col_axis, channel_axis, fill_mode, cval)\u001b[0m\n\u001b[1;32m    323\u001b[0m             \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfill_mode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m             cval=cval) for x_channel in x]\n\u001b[0m\u001b[1;32m    326\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchannel_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrollaxis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchannel_axis\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras_preprocessing/image.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    323\u001b[0m             \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfill_mode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m             cval=cval) for x_channel in x]\n\u001b[0m\u001b[1;32m    326\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchannel_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrollaxis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchannel_axis\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/scipy/ndimage/interpolation.py\u001b[0m in \u001b[0;36maffine_transform\u001b[0;34m(input, matrix, offset, output_shape, output, order, mode, cval, prefilter)\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m         _nd_image.geometric_transform(filtered, None, None, matrix, offset,\n\u001b[0;32m--> 486\u001b[0;31m                                       output, order, mode, cval, None, None)\n\u001b[0m\u001b[1;32m    487\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mreturn_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for x,y in validation_flow:\n",
    "    print(np.shape(x),np.shape(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 117 ms, sys: 60.5 ms, total: 177 ms\n",
      "Wall time: 155 ms\n"
     ]
    }
   ],
   "source": [
    "%time train_features = model_resnet.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 527 ms, sys: 64.3 ms, total: 591 ms\n",
      "Wall time: 459 ms\n"
     ]
    }
   ],
   "source": [
    "%time train_features = model_resnet.predict_generator(train_flow, steps=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.5 s, sys: 573 ms, total: 13.1 s\n",
      "Wall time: 12.8 s\n"
     ]
    }
   ],
   "source": [
    "%time test_features = model_resnet.predict_generator(test_flow, steps=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 7, 7, 2048)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time validation_features = model_resnet.predict_generator(validation_flow, steps=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_img = Input(shape=(224, 224, 3))\n",
    "\n",
    "model_resnet = ResNet50(weights='imagenet', include_top=False,\n",
    "    input_shape=(224, 224, 3),\n",
    "    input_tensor=input_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 47s, sys: 28.8 s, total: 3min 15s\n",
      "Wall time: 3min 9s\n"
     ]
    }
   ],
   "source": [
    "%time train_features = model_resnet.predict(preprocess_input(paths_to_tensor(train_files)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 20.4 s, sys: 3.01 s, total: 23.4 s\n",
      "Wall time: 22.6 s\n"
     ]
    }
   ],
   "source": [
    "%time test_features = model_resnet.predict(preprocess_input(paths_to_tensor(test_files)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 20.4 s, sys: 2.94 s, total: 23.3 s\n",
      "Wall time: 22.5 s\n"
     ]
    }
   ],
   "source": [
    "%time validation_features = model_resnet.predict(preprocess_input(paths_to_tensor(validation_files)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(16464, 7, 7, 2048), (2058, 7, 7, 2048), (2058, 7, 7, 2048)],\n",
       " [(16464, 120), (2058, 120), (2058, 120)]]"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[[np.shape(train_features), np.shape(test_features), np.shape(validation_features)],\n",
    " [np.shape(train_targets), np.shape(test_targets), np.shape(validation_targets)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('ResNet50/train_features', train_features)\n",
    "np.save('ResNet50/test_features', test_features)\n",
    "np.save('ResNet50/validation_features', validation_features)\n",
    "\n",
    "np.save('ResNet50/train_targets', train_targets)\n",
    "np.save('ResNet50/test_targets', test_targets)\n",
    "np.save('ResNet50/validation_targets', validation_targets)\n",
    "\n",
    "np.save('ResNet50/train_files',train_files)\n",
    "np.save('ResNet50/test_files',test_files)\n",
    "np.save('ResNet50/validation_files',validation_files)\n",
    "\n",
    "np.save('ResNet50/processed_train_files', processed_train_files)\n",
    "np.save('ResNet50/processed_test_files', processed_test_files)\n",
    "np.save('ResNet50/processed_validation_files.npy', processed_validation_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/keras_preprocessing/image.py:1183: UserWarning: Expected input to be images (as Numpy array) following the data format convention \"channels_last\" (channels on axis 3), i.e. expected either 1, 3 or 4 channels on axis 3. However, it was passed an array with shape (16464, 7, 7, 2048) (2048 channels).\n",
      "  ' channels).')\n",
      "/usr/local/lib/python3.5/dist-packages/keras_preprocessing/image.py:1183: UserWarning: Expected input to be images (as Numpy array) following the data format convention \"channels_last\" (channels on axis 3), i.e. expected either 1, 3 or 4 channels on axis 3. However, it was passed an array with shape (2058, 7, 7, 2048) (2048 channels).\n",
      "  ' channels).')\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/keras_preprocessing/image.py:1404: UserWarning: NumpyArrayIterator is set to use the data format convention \"channels_last\" (channels on axis 3), i.e. expected either 1, 3 or 4 channels on axis 3. However, it was passed an array with shape (16464, 7, 7, 2048) (2048 channels).\n",
      "  str(self.x.shape[channels_axis]) + ' channels).')\n",
      "/usr/local/lib/python3.5/dist-packages/keras_preprocessing/image.py:1404: UserWarning: NumpyArrayIterator is set to use the data format convention \"channels_last\" (channels on axis 3), i.e. expected either 1, 3 or 4 channels on axis 3. However, it was passed an array with shape (2058, 7, 7, 2048) (2048 channels).\n",
      "  str(self.x.shape[channels_axis]) + ' channels).')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "329/329 [==============================] - 764s 2s/step - loss: 1.4092 - categorical_accuracy: 0.6268 - val_loss: 0.9695 - val_categorical_accuracy: 0.7298\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.96954, saving model to ResNet50/weights.best.ResNet50.hdf5\n",
      "Epoch 2/50\n",
      "329/329 [==============================] - 770s 2s/step - loss: 0.5373 - categorical_accuracy: 0.8359 - val_loss: 0.9941 - val_categorical_accuracy: 0.7132\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.96954\n",
      "Epoch 3/50\n",
      "329/329 [==============================] - 774s 2s/step - loss: 0.3113 - categorical_accuracy: 0.9038 - val_loss: 0.9828 - val_categorical_accuracy: 0.7410\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.96954\n",
      "Epoch 4/50\n",
      "329/329 [==============================] - 779s 2s/step - loss: 0.1960 - categorical_accuracy: 0.9392 - val_loss: 0.9826 - val_categorical_accuracy: 0.7546\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.96954\n",
      "Epoch 5/50\n",
      "329/329 [==============================] - 786s 2s/step - loss: 0.1306 - categorical_accuracy: 0.9620 - val_loss: 1.0451 - val_categorical_accuracy: 0.7434\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.96954\n",
      "Epoch 6/50\n",
      "329/329 [==============================] - 790s 2s/step - loss: 0.1022 - categorical_accuracy: 0.9721 - val_loss: 1.0185 - val_categorical_accuracy: 0.7512\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.96954\n",
      "Epoch 7/50\n",
      "329/329 [==============================] - 796s 2s/step - loss: 0.0734 - categorical_accuracy: 0.9821 - val_loss: 1.0133 - val_categorical_accuracy: 0.7634\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.96954\n",
      "Epoch 8/50\n",
      "329/329 [==============================] - 806s 2s/step - loss: 0.0663 - categorical_accuracy: 0.9830 - val_loss: 1.0314 - val_categorical_accuracy: 0.7566\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.96954\n",
      "Epoch 9/50\n",
      "329/329 [==============================] - 805s 2s/step - loss: 0.0612 - categorical_accuracy: 0.9850 - val_loss: 1.1185 - val_categorical_accuracy: 0.7522\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.96954\n",
      "Epoch 10/50\n",
      "329/329 [==============================] - 811s 2s/step - loss: 0.0722 - categorical_accuracy: 0.9801 - val_loss: 1.2496 - val_categorical_accuracy: 0.7322\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.96954\n",
      "Epoch 11/50\n",
      "329/329 [==============================] - 813s 2s/step - loss: 0.1020 - categorical_accuracy: 0.9700 - val_loss: 1.2858 - val_categorical_accuracy: 0.7532\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.96954\n",
      "Epoch 12/50\n",
      "329/329 [==============================] - 816s 2s/step - loss: 0.0996 - categorical_accuracy: 0.9704 - val_loss: 1.3722 - val_categorical_accuracy: 0.7400\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.96954\n",
      "Epoch 13/50\n",
      "329/329 [==============================] - 811s 2s/step - loss: 0.0957 - categorical_accuracy: 0.9691 - val_loss: 1.3859 - val_categorical_accuracy: 0.7351\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.96954\n",
      "Epoch 14/50\n",
      "329/329 [==============================] - 816s 2s/step - loss: 0.0689 - categorical_accuracy: 0.9791 - val_loss: 1.4301 - val_categorical_accuracy: 0.7459\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.96954\n",
      "Epoch 15/50\n",
      "329/329 [==============================] - 817s 2s/step - loss: 0.0640 - categorical_accuracy: 0.9790 - val_loss: 1.4381 - val_categorical_accuracy: 0.7507\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.96954\n",
      "Epoch 16/50\n",
      "329/329 [==============================] - 813s 2s/step - loss: 0.0633 - categorical_accuracy: 0.9815 - val_loss: 1.4933 - val_categorical_accuracy: 0.7439\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.96954\n",
      "Epoch 17/50\n",
      "329/329 [==============================] - 815s 2s/step - loss: 0.0630 - categorical_accuracy: 0.9826 - val_loss: 1.3992 - val_categorical_accuracy: 0.7585\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.96954\n",
      "Epoch 18/50\n",
      "329/329 [==============================] - 815s 2s/step - loss: 0.0542 - categorical_accuracy: 0.9850 - val_loss: 1.4860 - val_categorical_accuracy: 0.7532\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.96954\n",
      "Epoch 19/50\n",
      "329/329 [==============================] - 815s 2s/step - loss: 0.0466 - categorical_accuracy: 0.9855 - val_loss: 1.6554 - val_categorical_accuracy: 0.7444\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.96954\n",
      "Epoch 20/50\n",
      "329/329 [==============================] - 827s 3s/step - loss: 0.0601 - categorical_accuracy: 0.9832 - val_loss: 1.6546 - val_categorical_accuracy: 0.7395\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.96954\n",
      "Epoch 21/50\n",
      "329/329 [==============================] - 819s 2s/step - loss: 0.0655 - categorical_accuracy: 0.9805 - val_loss: 1.7544 - val_categorical_accuracy: 0.7380\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.96954\n",
      "Epoch 22/50\n",
      "329/329 [==============================] - 821s 2s/step - loss: 0.0706 - categorical_accuracy: 0.9790 - val_loss: 2.0665 - val_categorical_accuracy: 0.7200\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.96954\n",
      "Epoch 23/50\n",
      "329/329 [==============================] - 819s 2s/step - loss: 0.0768 - categorical_accuracy: 0.9778 - val_loss: 1.8393 - val_categorical_accuracy: 0.7390\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.96954\n",
      "Epoch 24/50\n",
      "329/329 [==============================] - 820s 2s/step - loss: 0.0547 - categorical_accuracy: 0.9844 - val_loss: 1.7934 - val_categorical_accuracy: 0.7400\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.96954\n",
      "Epoch 25/50\n",
      "329/329 [==============================] - 819s 2s/step - loss: 0.0527 - categorical_accuracy: 0.9853 - val_loss: 1.6791 - val_categorical_accuracy: 0.7502\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.96954\n",
      "Epoch 26/50\n",
      "329/329 [==============================] - 812s 2s/step - loss: 0.0456 - categorical_accuracy: 0.9871 - val_loss: 1.8702 - val_categorical_accuracy: 0.7302\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.96954\n",
      "Epoch 27/50\n",
      "329/329 [==============================] - 819s 2s/step - loss: 0.0574 - categorical_accuracy: 0.9843 - val_loss: 1.8624 - val_categorical_accuracy: 0.7346\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.96954\n",
      "Epoch 28/50\n",
      "329/329 [==============================] - 819s 2s/step - loss: 0.0523 - categorical_accuracy: 0.9860 - val_loss: 1.9012 - val_categorical_accuracy: 0.7463\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.96954\n",
      "Epoch 29/50\n",
      "329/329 [==============================] - 816s 2s/step - loss: 0.0532 - categorical_accuracy: 0.9860 - val_loss: 1.9244 - val_categorical_accuracy: 0.7395\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.96954\n",
      "Epoch 30/50\n",
      "329/329 [==============================] - 820s 2s/step - loss: 0.0529 - categorical_accuracy: 0.9858 - val_loss: 1.7940 - val_categorical_accuracy: 0.7551\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.96954\n",
      "Epoch 31/50\n",
      "329/329 [==============================] - 818s 2s/step - loss: 0.0501 - categorical_accuracy: 0.9861 - val_loss: 1.8881 - val_categorical_accuracy: 0.7405\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.96954\n",
      "Epoch 32/50\n",
      "329/329 [==============================] - 820s 2s/step - loss: 0.0476 - categorical_accuracy: 0.9860 - val_loss: 1.9272 - val_categorical_accuracy: 0.7400\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.96954\n",
      "Epoch 33/50\n",
      "329/329 [==============================] - 814s 2s/step - loss: 0.0451 - categorical_accuracy: 0.9877 - val_loss: 1.9529 - val_categorical_accuracy: 0.7356\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.96954\n",
      "Epoch 34/50\n",
      "329/329 [==============================] - 820s 2s/step - loss: 0.0556 - categorical_accuracy: 0.9852 - val_loss: 1.9802 - val_categorical_accuracy: 0.7376\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.96954\n",
      "Epoch 35/50\n",
      "329/329 [==============================] - 822s 2s/step - loss: 0.0585 - categorical_accuracy: 0.9855 - val_loss: 2.0158 - val_categorical_accuracy: 0.7439\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.96954\n",
      "Epoch 36/50\n",
      "329/329 [==============================] - 820s 2s/step - loss: 0.0615 - categorical_accuracy: 0.9844 - val_loss: 2.0666 - val_categorical_accuracy: 0.7332\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.96954\n",
      "Epoch 37/50\n",
      "329/329 [==============================] - 821s 2s/step - loss: 0.0436 - categorical_accuracy: 0.9897 - val_loss: 2.0150 - val_categorical_accuracy: 0.7390\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.96954\n",
      "Epoch 38/50\n",
      "329/329 [==============================] - 816s 2s/step - loss: 0.0389 - categorical_accuracy: 0.9907 - val_loss: 2.0477 - val_categorical_accuracy: 0.7410\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.96954\n",
      "Epoch 39/50\n",
      "329/329 [==============================] - 820s 2s/step - loss: 0.0642 - categorical_accuracy: 0.9849 - val_loss: 2.0818 - val_categorical_accuracy: 0.7380\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.96954\n",
      "Epoch 40/50\n",
      "329/329 [==============================] - 823s 3s/step - loss: 0.0596 - categorical_accuracy: 0.9837 - val_loss: 2.0163 - val_categorical_accuracy: 0.7527\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.96954\n",
      "Epoch 41/50\n",
      "329/329 [==============================] - 821s 2s/step - loss: 0.0434 - categorical_accuracy: 0.9897 - val_loss: 2.1398 - val_categorical_accuracy: 0.7376\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.96954\n",
      "Epoch 42/50\n",
      "329/329 [==============================] - 822s 2s/step - loss: 0.0485 - categorical_accuracy: 0.9883 - val_loss: 2.1078 - val_categorical_accuracy: 0.7376\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.96954\n",
      "Epoch 43/50\n",
      "329/329 [==============================] - 823s 3s/step - loss: 0.0407 - categorical_accuracy: 0.9895 - val_loss: 2.1171 - val_categorical_accuracy: 0.7444\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.96954\n",
      "Epoch 44/50\n",
      "329/329 [==============================] - 819s 2s/step - loss: 0.0508 - categorical_accuracy: 0.9875 - val_loss: 2.1692 - val_categorical_accuracy: 0.7380\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.96954\n",
      "Epoch 45/50\n",
      "329/329 [==============================] - 820s 2s/step - loss: 0.0605 - categorical_accuracy: 0.9857 - val_loss: 2.0885 - val_categorical_accuracy: 0.7420\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.96954\n",
      "Epoch 46/50\n",
      "329/329 [==============================] - 820s 2s/step - loss: 0.0384 - categorical_accuracy: 0.9893 - val_loss: 2.1076 - val_categorical_accuracy: 0.7454\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.96954\n",
      "Epoch 47/50\n",
      "329/329 [==============================] - 822s 2s/step - loss: 0.0412 - categorical_accuracy: 0.9894 - val_loss: 2.1816 - val_categorical_accuracy: 0.7429\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.96954\n",
      "Epoch 48/50\n",
      "329/329 [==============================] - 822s 2s/step - loss: 0.0376 - categorical_accuracy: 0.9898 - val_loss: 2.1330 - val_categorical_accuracy: 0.7400\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.96954\n",
      "Epoch 49/50\n",
      "329/329 [==============================] - 823s 3s/step - loss: 0.0404 - categorical_accuracy: 0.9911 - val_loss: 2.1472 - val_categorical_accuracy: 0.7478\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.96954\n",
      "Epoch 50/50\n",
      "329/329 [==============================] - 827s 3s/step - loss: 0.0357 - categorical_accuracy: 0.9922 - val_loss: 2.0480 - val_categorical_accuracy: 0.7493\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.96954\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3a4f0642b0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 50\n",
    "epochs = 50\n",
    "\n",
    "Resnet50_model.fit_generator(datagen_train.flow(train_features, train_targets, batch_size=batch_size),\n",
    "                    steps_per_epoch=train_features.shape[0] // batch_size,\n",
    "                    epochs=epochs, callbacks=[checkpointer_resnet, tb],\n",
    "                    validation_data=datagen_valid.flow(validation_features, validation_targets, batch_size=batch_size),\n",
    "                    validation_steps= validation_features.shape[0] // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Compile the model.\n",
    "Resnet50_model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "Resnet50_model.compile(optimizer=keras.optimizers.Nadam(),\n",
    "                  loss=keras.losses.categorical_crossentropy,\n",
    "                  metrics=[keras.metrics.categorical_accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Train the model.\n",
    "checkpointer_resnet = ModelCheckpoint(filepath='ResNet50/weights.best.ResNet50.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "\n",
    "tb = keras.callbacks.TensorBoard(log_dir='./logs'+'/'+id_generator(), histogram_freq=0, batch_size=32, write_graph=True, write_grads=False, write_images=False, embeddings_freq=0, embeddings_layer_names=None, embeddings_metadata=None, embeddings_data=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_loss improved from inf to 2.97892, saving model to ResNet50/weights.best.ResNet50.hdf5\n",
      "\n",
      "Epoch 00002: val_loss improved from 2.97892 to 2.96905, saving model to ResNet50/weights.best.ResNet50.hdf5\n",
      "\n",
      "Epoch 00003: val_loss improved from 2.96905 to 2.91374, saving model to ResNet50/weights.best.ResNet50.hdf5\n",
      "\n",
      "Epoch 00004: val_loss improved from 2.91374 to 2.89610, saving model to ResNet50/weights.best.ResNet50.hdf5\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 2.89610\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 2.89610\n",
      "\n",
      "Epoch 00007: val_loss improved from 2.89610 to 2.79224, saving model to ResNet50/weights.best.ResNet50.hdf5\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 2.79224\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 2.79224\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 2.79224\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 2.79224\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 2.79224\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 2.79224\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 2.79224\n",
      "\n",
      "Epoch 00015: val_loss improved from 2.79224 to 2.74174, saving model to ResNet50/weights.best.ResNet50.hdf5\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 2.74174\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 2.74174\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 2.74174\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 2.74174\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 2.74174\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 2.74174\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 2.74174\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 2.74174\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 2.74174\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 2.74174\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 2.74174\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 2.74174\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 2.74174\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 2.74174\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 2.74174\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 2.74174\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 2.74174\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 2.74174\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 2.74174\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 2.74174\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 2.74174\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 2.74174\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 2.74174\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 2.74174\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 2.74174\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 2.74174\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 2.74174\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 2.74174\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 2.74174\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 2.74174\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 2.74174\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 2.74174\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 2.74174\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 2.74174\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 2.74174\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 2.74174\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 2.74174\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 2.74174\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 2.74174\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 2.74174\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 2.74174\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 2.74174\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 2.74174\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 2.74174\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 2.74174\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 2.74174\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 2.74174\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 2.74174\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 2.74174\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 2.74174\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 2.74174\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 2.74174\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 2.74174\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 2.74174\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 2.74174\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 2.74174\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 2.74174\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-10bae14fc0c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m Resnet50_model.fit(train_features, train_targets, \n\u001b[1;32m      2\u001b[0m           \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_targets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m           epochs=500, batch_size=50, callbacks=[checkpointer_resnet, tb], verbose = 0)\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1040\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1042\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1043\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1044\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2659\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2660\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2661\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2662\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2663\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2629\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2630\u001b[0m                                 session)\n\u001b[0;32m-> 2631\u001b[0;31m         \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2632\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1449\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_with_new_api\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1450\u001b[0m           return tf_session.TF_SessionRunCallable(\n\u001b[0;32m-> 1451\u001b[0;31m               self._session._session, self._handle, args, status, None)\n\u001b[0m\u001b[1;32m   1452\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1453\u001b[0m           return tf_session.TF_DeprecatedSessionRunCallable(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "Resnet50_model.fit(train_features, train_targets, \n",
    "          validation_data=(validation_features, validation_targets),\n",
    "          epochs=100, batch_size=50, callbacks=[checkpointer_resnet, tb], verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load the model weights with the best validation loss.\n",
    "Resnet50_model.load_weights('ResNet50/weights.best.ResNet50.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 74.7328%\n"
     ]
    }
   ],
   "source": [
    "### Calculate classification accuracy on the test dataset.\n",
    "predictions = np.argmax(Resnet50_model.predict(test_features), axis=1)\n",
    "actual = np.argmax(test_targets, axis=1)\n",
    "# Report test accuracy\n",
    "test_accuracy = 100*np.sum(predictions==actual)/len(predictions)\n",
    "print('Test accuracy: %.4f%%' % test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 75.6074%\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "Resnet50_model.compile(optimizer=keras.optimizers.Nadam(),\n",
    "                  loss=keras.losses.categorical_crossentropy,\n",
    "                  metrics=[keras.metrics.categorical_accuracy])\n",
    "### Train the model.\n",
    "checkpointer_resnet = ModelCheckpoint(filepath='ResNet50/weights.best.ResNet50.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "\n",
    "tb = keras.callbacks.TensorBoard(log_dir='./logs'+'/'+id_generator(), histogram_freq=0, batch_size=32, write_graph=True, write_grads=False, write_images=False, embeddings_freq=0, embeddings_layer_names=None, embeddings_metadata=None, embeddings_data=None)\n",
    "\n",
    "for x,y in train_flow:\n",
    "    features = model_resnet.predict(x)\n",
    "    history = Resnet50_model.fit(features, y, callbacks=[checkpointer_resnet, tb], validation_split=float(.1*9/10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 29 samples, validate on 3 samples\n",
      "Epoch 1/1\n",
      "29/29 [==============================] - 0s 523us/step - loss: 1.9773 - categorical_accuracy: 0.6552 - val_loss: 0.6282 - val_categorical_accuracy: 0.6667\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.05890\n",
      "Train on 29 samples, validate on 3 samples\n",
      "Epoch 1/1\n",
      "29/29 [==============================] - 0s 318us/step - loss: 1.0658 - categorical_accuracy: 0.7586 - val_loss: 0.0024 - val_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.05890 to 0.00240, saving model to ResNet50/weights.best.ResNet50.hdf5\n",
      "Train on 29 samples, validate on 3 samples\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-68-aefc36d1e60d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_flow\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_resnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mResnet50_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcheckpointer_resnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m.1\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1040\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1042\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1043\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1044\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0mcallback_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m     callbacks.set_params({\n\u001b[1;32m    117\u001b[0m         \u001b[0;34m'batch_size'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mset_model\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m             \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_epoch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mset_model\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    806\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_graph\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    807\u001b[0m             self.writer = tf.summary.FileWriter(self.log_dir,\n\u001b[0;32m--> 808\u001b[0;31m                                                 self.sess.graph)\n\u001b[0m\u001b[1;32m    809\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFileWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/summary/writer/writer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, logdir, graph, max_queue, flush_secs, graph_def, filename_suffix)\u001b[0m\n\u001b[1;32m    351\u001b[0m     event_writer = EventFileWriter(logdir, max_queue, flush_secs,\n\u001b[1;32m    352\u001b[0m                                    filename_suffix)\n\u001b[0;32m--> 353\u001b[0;31m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFileWriter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent_writer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph_def\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for x,y in train_flow:\n",
    "    features = model_resnet.predict(x)\n",
    "    history = Resnet50_model.fit(features, y, callbacks=[checkpointer_resnet, tb], validation_split=float(.1*9/10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'categorical_accuracy': [0.7586206793785095],\n",
       " 'loss': [1.06578528881073],\n",
       " 'val_categorical_accuracy': [1.0],\n",
       " 'val_loss': [0.002400173107162118]}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-7b10bcabfabe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalidation_flow\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_resnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mResnet50_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1040\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1042\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1043\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1044\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0mindices_for_conversion_to_dense\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_sparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m             \u001b[0mindices_for_conversion_to_dense\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "for x,y in validation_flow:\n",
    "    features = model_resnet.predict(x)\n",
    "    Resnet50_model.fit(epochs=10, steps_per_epoch=10, validation_steps=10, validation_data=(features, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
